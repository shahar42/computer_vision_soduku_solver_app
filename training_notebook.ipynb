# Cell 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Cell 2: Clone Repo and Install Dependencies
!git clone https://github.com/shahar42/computer_vision_soduku_solver_app.git
# %cd computer_vision_soduku_solver_app

!pip install -r requirements.txt
!pip install scikit-learn scikit-image opencv-python-headless tensorflow

# Commented out IPython magic to ensure Python compatibility.
# %cd computer_vision_soduku_solver_app

# Cell 3: All Imports, Path Setup, Global Config
import os
import sys
import numpy as np
import cv2
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
import re
import random
import glob
from tqdm import tqdm
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from collections import defaultdict
from sklearn.model_selection import train_test_split

# Add project to sys.path
sys.path.append('/content/computer_vision_soduku_solver_app')

# Import project-specific modules (ensure these are found after sys.path update)
from models.board_detector import BoardDetector
# RobustIntersectionDetector class might not be used directly if test_detector Keras model is used,
# but keeping the import in case any of its utility methods were planned for use.
from models.intersection_detector import RobustIntersectionDetector
from models.grid_reconstructor import RobustGridReconstructor
from models.cell_extractor import RobustCellExtractor

# --- Directories and Global Settings ---
DATA_DIR_INTERSECTIONS = "/content/drive/MyDrive/Soduku_project/SODUKU_IMG/intersection_dara_x,y"
DATA_DIR_BOARD_DETECTOR = "/content/drive/MyDrive/Soduku_project/SODUKU_IMG/intersection_dara_x,y" # Same as above, as per original
TEST_IMAGE_DIR = "/content/drive/MyDrive/Soduku_project/SODUKU_IMG/testing_images"

OUTPUT_DIR_GENERAL = "/content/processed_data" # General output for any processed items
MODEL_DIR = "/content/models" # For locally trained models during session
SAVED_MODELS_DRIVE_DIR = "/content/drive/MyDrive/Soduku_project/trained_models" # For final model storage on Drive

os.makedirs(OUTPUT_DIR_GENERAL, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(SAVED_MODELS_DRIVE_DIR, exist_ok=True)

# Load project configuration to check patch_size settings for Intersection Detector
patch_size_intersection = 15 # Default
try:
    from config.settings import get_settings
    settings = get_settings()
    patch_size_intersection = settings.get_nested("intersection_detector").get("patch_size", 15)
    print(f"üìä Project settings for Intersection Detector: patch_size={patch_size_intersection}")
except Exception as e:
    print(f"‚ö†Ô∏è Could not load project settings for patch_size: {e}. Using default patch_size={patch_size_intersection}")

print(f"‚úÖ Environment setup complete")
print(f"üß† TensorFlow version: {tf.__version__}")
print(f"üñ•Ô∏è GPU available: {tf.config.list_physical_devices('GPU')}")
print(f"üìÇ Intersection Data directory: {DATA_DIR_INTERSECTIONS}")
print(f"üìÇ Board Detector Data directory: {DATA_DIR_BOARD_DETECTOR}")
print(f"üíæ Models will be saved in: {MODEL_DIR} (local) and {SAVED_MODELS_DRIVE_DIR} (Drive)")

# Cell 4: Utilities
# --- General File Parsing (for Intersection data) ---
def parse_intersection_file(file_path):
    with open(file_path, 'r') as f:
        lines = f.readlines()
    points = []
    for line in lines[2:]:
        if line.strip():
            try:
                x, y = map(int, line.strip().split())
                points.append((x, y))
            except ValueError:
                print(f"Warning: Could not parse line: {line} in file {file_path}")
    return points

def load_intersection_data(data_dir):
    dat_files = glob.glob(os.path.join(data_dir, "*.dat"))
    images = []
    points_list = []
    for dat_file in tqdm(dat_files, desc="Loading intersection data"):
        base_name = os.path.splitext(os.path.basename(dat_file))[0]
        img_file = None
        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:
            potential_img = os.path.join(data_dir, base_name + ext)
            if os.path.exists(potential_img):
                img_file = potential_img
                break
        if img_file is None:
            print(f"Warning: No image found for {dat_file}")
            continue
        img = cv2.imread(img_file)
        if img is None:
            print(f"Warning: Could not load image {img_file}")
            continue
        points = parse_intersection_file(dat_file)
        height, width = img.shape[:2]
        valid_points = [(x, y) for x, y in points if 0 <= x < width and 0 <= y < height]
        if len(valid_points) < len(points):
            print(f"Warning: {len(points) - len(valid_points)} points were outside image bounds in {dat_file}")
        if valid_points:
            images.append(img)
            points_list.append(valid_points)
    print(f"‚úÖ Loaded {len(images)} images with intersection annotations for intersection model.")
    return images, points_list

# --- Augmentation for Intersection Detector Training ---
def custom_augment_intersection(image):
    original_shape = image.shape
    if len(image.shape) == 3 and image.shape[-1] == 1:
        image = np.squeeze(image, axis=-1)
    if image.max() <= 1.0:
        image = (image * 255.0).astype(np.uint8)
    else:
        image = image.astype(np.uint8)

    if random.random() < 0.4: # Brightness
        brightness_factor = random.uniform(0.6, 1.2)
        image = np.clip(image.astype(np.float32) * brightness_factor, 0, 255).astype(np.uint8)
    if random.random() < 0.3: # Noise
        noise_type = random.choice(['gaussian', 'salt_pepper'])
        if noise_type == 'gaussian':
            noise_val = np.random.normal(0, random.uniform(5, 15), image.shape)
            image = np.clip(image.astype(np.float32) + noise_val, 0, 255).astype(np.uint8)
        elif noise_type == 'salt_pepper':
            noise_ratio = random.uniform(0.01, 0.05)
            salt_pepper = np.random.random(image.shape)
            image[salt_pepper < noise_ratio/2] = 0
            image[salt_pepper > 1 - noise_ratio/2] = 255
    if random.random() < 0.3: # Blur/Sharpen
        blur_type = random.choice(['blur', 'sharpen'])
        if blur_type == 'blur':
            kernel_size = random.choice([3, 5])
            sigma = random.uniform(0.5, 1.5)
            image = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)
        elif blur_type == 'sharpen':
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            image = cv2.filter2D(image, -1, kernel)
            image = np.clip(image, 0, 255).astype(np.uint8)
    image = image.astype(np.float32) / 255.0
    if len(original_shape) == 3 and original_shape[-1] == 1:
        image = np.expand_dims(image, axis=-1)
    return image

# --- Helper Functions for Board Detector Data ---
def parse_dat_file_board(dat_path): # Renamed to avoid conflict if structure differs subtly
    with open(dat_path, 'r') as f: lines = f.readlines()
    points = []
    for line in lines[2:]:
        if line.strip():
            try: x, y = map(int, line.strip().split()); points.append((x, y))
            except ValueError: print(f"Warning: Could not parse line: {line.strip()} in {dat_path}")
    return points

def detect_grid_corners_board(points):
    if len(points) < 4: raise ValueError(f"Need at least 4 points for corner detection, got {len(points)}")
    points_array = np.array(points)
    sum_coords = points_array[:, 0] + points_array[:, 1]
    diff_coords = points_array[:, 1] - points_array[:, 0]
    tl_idx, tr_idx, bl_idx, br_idx = np.argmin(sum_coords), np.argmin(diff_coords), np.argmax(diff_coords), np.argmax(sum_coords)
    return {
        'tl': tuple(points[tl_idx]), 'tr': tuple(points[tr_idx]),
        'br': tuple(points[br_idx]), 'bl': tuple(points[bl_idx])
    }

def create_bounding_box_board(corners, img_width, img_height, padding=0.05):
    x_coords = [corners[k][0] for k in corners]
    y_coords = [corners[k][1] for k in corners]
    min_x, max_x, min_y, max_y = min(x_coords), max(x_coords), min(y_coords), max(y_coords)
    width, height = max_x - min_x, max_y - min_y
    pad_x, pad_y = width * padding, height * padding
    x1, y1 = max(0, min_x - pad_x), max(0, min_y - pad_y)
    x2, y2 = min(img_width, max_x + pad_x), min(img_height, max_y + pad_y)
    return (x1 / img_width, y1 / img_height, x2 / img_width, y2 / img_height) # Normalized

# --- Augmentation Functions for Board Detector Training ---
def apply_gaussian_noise_board(image, noise_intensity=0.02):
    noise = np.random.normal(0, noise_intensity * 255, image.shape)
    return np.clip(image.astype(np.float32) + noise, 0, 255).astype(np.uint8)

def apply_blur_augmentation_board(image, blur_range=(0.5, 2.0)):
    blur_type = np.random.choice(['gaussian', 'motion'])
    kernel_size_param = np.random.uniform(blur_range[0], blur_range[1])
    if blur_type == 'gaussian':
        if kernel_size_param > 0.1: return cv2.GaussianBlur(image, (0, 0), kernel_size_param)
        return image.copy()
    else: # motion
        kernel_size_int = max(3, int(kernel_size_param * 3)); kernel_size_int += (kernel_size_int % 2 == 0)
        angle = np.random.uniform(0, 180)
        kernel = np.zeros((kernel_size_int, kernel_size_int)); kernel[kernel_size_int//2, :] = 1
        kernel = cv2.warpAffine(kernel, cv2.getRotationMatrix2D((kernel_size_int//2, kernel_size_int//2), angle, 1.0), (kernel_size_int, kernel_size_int))
        return cv2.filter2D(image, -1, kernel / kernel_size_int) # Normalize kernel

def apply_contrast_augmentation_board(image, contrast_range=(0.7, 1.4)):
    factor = np.random.uniform(contrast_range[0], contrast_range[1])
    return np.clip((image.astype(np.float32) - 128) * factor + 128, 0, 255).astype(np.uint8)

def apply_gamma_correction_board(image, gamma_range=(0.8, 1.2)):
    gamma = np.random.uniform(gamma_range[0], gamma_range[1])
    return np.clip(255 * ((image / 255.0) ** gamma), 0, 255).astype(np.uint8)

def apply_brightness_augmentation_board(image, brightness_range=(0.6, 1.4)):
    factor = np.random.uniform(brightness_range[0], brightness_range[1])
    return np.clip(image.astype(np.float32) * factor, 0, 255).astype(np.uint8)

def apply_zoom_augmentation_board(image, bbox, zoom_range=(0.7, 1.3)):
    zoom_factor = np.random.uniform(zoom_range[0], zoom_range[1])
    h, w = image.shape[:2]; new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)
    x1_orig_px, y1_orig_px, x2_orig_px, y2_orig_px = bbox[0]*w, bbox[1]*h, bbox[2]*w, bbox[3]*h

    if zoom_factor > 1.0: # Zoom in (crop)
        resized = cv2.resize(image, (new_w, new_h))
        start_x, start_y = (new_w - w) // 2, (new_h - h) // 2
        cropped = resized[start_y:start_y+h, start_x:start_x+w]
        x1_new = (x1_orig_px * zoom_factor - start_x) / w
        y1_new = (y1_orig_px * zoom_factor - start_y) / h
        x2_new = (x2_orig_px * zoom_factor - start_x) / w
        y2_new = (y2_orig_px * zoom_factor - start_y) / h
        final_img = cropped
    else: # Zoom out (pad)
        resized = cv2.resize(image, (new_w, new_h))
        padded = np.zeros((h, w, 3), dtype=image.dtype)
        start_x, start_y = (w - new_w) // 2, (h - new_h) // 2
        padded[start_y:start_y+new_h, start_x:start_x+new_w] = resized
        x1_new = (x1_orig_px * zoom_factor + start_x) / w
        y1_new = (y1_orig_px * zoom_factor + start_y) / h
        x2_new = (x2_orig_px * zoom_factor + start_x) / w
        y2_new = (y2_orig_px * zoom_factor + start_y) / h
        final_img = padded
    
    adj_bbox = (max(0,min(1,x1_new)), max(0,min(1,y1_new)), max(0,min(1,x2_new)), max(0,min(1,y2_new)))
    return final_img, adj_bbox

# --- Multi-Grid Separation ---
def separate_sudoku_grids(points, min_grid_size=40):
    if not points or len(points) < min_grid_size: return [points] if points else []
    points_array = np.array(points)
    if len(points_array) <= 1: return [points] # Avoid error with NearestNeighbors if too few points

    # Ensure n_neighbors is less than or equal to the number of samples
    k_neighbors = min(8, len(points_array) -1)
    if k_neighbors <=0 : return [points] # Not enough points to form pairs for distance calculation

    nbrs = NearestNeighbors(n_neighbors=k_neighbors).fit(points_array)
    distances, _ = nbrs.kneighbors(points_array)
    grid_scale = np.median(distances[:, min(2, k_neighbors-1)]) # Use 3rd or available nearest
    
    detection_ratio = min(1.0, len(points) / 100.0)
    adaptive_gap_factor = 1.0 + (detection_ratio / 100.0) + (0.8 / (detection_ratio + 1e-6)) # Avoid division by zero

    adjacency = defaultdict(set)
    for i, (x1, y1) in enumerate(points):
        for j, (x2, y2) in enumerate(points):
            if i == j: continue
            dx, dy = x2 - x1, y2 - y1; dist = np.sqrt(dx*dx + dy*dy)
            if dist > grid_scale * adaptive_gap_factor or dist == 0: continue
            unit_dx, unit_dy = dx/dist, dy/dist
            if abs(unit_dx) > 0.9 or abs(unit_dy) > 0.9: # Horizontal or Vertical
                adjacency[i].add(j); adjacency[j].add(i)
    
    visited = set(); grids = []
    for i in range(len(points)):
        if i in visited: continue
        grid_indices = []; queue = [i]
        while queue:
            current = queue.pop(0)
            if current in visited: continue
            visited.add(current); grid_indices.append(current)
            for neighbor in adjacency[current]:
                if neighbor not in visited: queue.append(neighbor)
        if len(grid_indices) >= min_grid_size: grids.append([points[idx] for idx in grid_indices])
    
    print(f"Found {len(grids)} separate grid(s) by separation logic.")
    return grids if grids else [points] # Return original if no separation occurs

# --- Patched Cell Extractor ---
def fixed_extract_cell(self, image, grid_points): # Renamed to avoid potential namespace collision
    self.perspective_extractor.extraction_mode = "preserve"
    self.perspective_extractor.adaptive_thresholding = False
    self.perspective_extractor.contrast_enhancement = False
    self.perspective_extractor.noise_reduction = False
    self.perspective_extractor.border_padding = 0.1 # As per d_cell 12
    self.use_multiple_extractors = False
    return self.perspective_extractor.extract(image, grid_points)

RobustCellExtractor.extract = fixed_extract_cell # Apply patch
print("‚úÖ RobustCellExtractor patched successfully to use perspective extraction with grayscale preservation.")

# --- Intersection Detection Test Function (from d_cell 12 / last cell) ---
def test_detector_intersections(image, model, patch_size=15, confidence_threshold=0.8, x_correction=2, y_correction=2):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image.copy()
    normalized = gray.astype(np.float32) / 255.0
    stride = patch_size // 4  # As per d_cell 12
    half_patch = patch_size // 2
    model_input_size = 32 # Standard input for intersection model

    padded = np.pad(normalized, ((half_patch, half_patch), (half_patch, half_patch)), mode='constant')
    height, width = normalized.shape
    patches, positions = [], []

    for y_pos in range(0, height, stride):
        for x_pos in range(0, width, stride):
            patch = padded[y_pos : y_pos + patch_size, x_pos : x_pos + patch_size]
            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:
                patches.append(cv2.resize(patch, (model_input_size, model_input_size)))
                positions.append((x_pos + half_patch, y_pos + half_patch))
    
    detected_points, confidences_list = [], []
    if patches:
        patches_array = np.array(patches).reshape(-1, model_input_size, model_input_size, 1)
        batch_s = 128
        for i in range(0, len(patches_array), batch_s):
            batch_preds = model.predict(patches_array[i:i+batch_s], verbose=0)
            for j_pred, conf_pred in enumerate(batch_preds):
                if conf_pred[0] >= confidence_threshold:
                    detected_points.append(positions[i+j_pred])
                    confidences_list.append(float(conf_pred[0]))
    
    if not detected_points: return []
    
    points_arr = np.array(detected_points)
    confidences_arr = np.array(confidences_list)
    # Ensure eps is reasonable, e.g., patch_size related, min_samples=1 for inclusive clustering
    clustering = DBSCAN(eps=patch_size, min_samples=1).fit(points_arr)
    labels = clustering.labels_
    
    clustered_points = []
    for cluster_id in np.unique(labels):
        if cluster_id == -1: continue # Skip noise points if DBSCAN produces them (not with min_samples=1)
        cluster_indices = np.where(labels == cluster_id)[0]
        if len(cluster_indices) == 0: continue
        
        cluster_confidences = confidences_arr[cluster_indices]
        weights = cluster_confidences / np.sum(cluster_confidences)
        center_x = np.sum(points_arr[cluster_indices, 0] * weights)
        center_y = np.sum(points_arr[cluster_indices, 1] * weights)
        
        # Apply empirical correction
        corrected_x = int(center_x + x_correction)
        corrected_y = int(center_y - y_correction) # y-correction was positive in original meaning subtract from image y
        clustered_points.append((corrected_x, corrected_y))
            
    return clustered_points

print("‚úÖ Utilities cell executed.")

# Cell 5: Intersection Detector - Data Preparation Function
def prepare_training_data_intersection(images, points_list, patch_size=15, model_input_size=32):
    X_train, y_train = [], []
    half_patch = patch_size // 2

    for i, (img, points) in enumerate(zip(images, points_list)):
        try:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img.copy()
            gray = gray.astype(np.float32) / 255.0
            height, width = gray.shape

            # Positive samples
            num_pos_added = 0
            for x, y in points:
                top, bottom = y - half_patch, y + half_patch + (patch_size % 2)
                left, right = x - half_patch, x + half_patch + (patch_size % 2)
                if top >= 0 and bottom <= height and left >= 0 and right <= width:
                    patch = gray[top:bottom, left:right]
                    if patch.shape[0] == patch_size and patch.shape[1] == patch_size:
                        X_train.append(cv2.resize(patch, (model_input_size, model_input_size)))
                        y_train.append(1); num_pos_added+=1
            
            # Negative samples
            num_negative = min(num_pos_added * 3, 300)
            mask = np.ones(gray.shape, dtype=np.uint8)
            no_go_radius = patch_size 
            for x, y in points: cv2.circle(mask, (x, y), no_go_radius, 0, -1)
            
            sampled_neg_count = 0
            for _ in range(num_negative * 5): # More attempts to find negatives
                if sampled_neg_count >= num_negative: break
                rand_x = random.randint(half_patch, width - half_patch - (patch_size % 2) -1)
                rand_y = random.randint(half_patch, height - half_patch - (patch_size % 2) -1)
                if mask[rand_y, rand_x] == 1:
                    top, bottom = rand_y - half_patch, rand_y + half_patch + (patch_size % 2)
                    left, right = rand_x - half_patch, rand_x + half_patch + (patch_size % 2)
                    patch = gray[top:bottom, left:right]
                    if patch.shape[0] == patch_size and patch.shape[1] == patch_size:
                        X_train.append(cv2.resize(patch, (model_input_size, model_input_size)))
                        y_train.append(0); sampled_neg_count+=1
        except Exception as e: print(f"Error processing image {i} for intersection training data: {str(e)}")

    X_train_arr = np.array(X_train).reshape(-1, model_input_size, model_input_size, 1) if X_train else np.array([])
    y_train_arr = np.array(y_train)
    
    if len(X_train_arr) > 0 :
        print(f"‚úÖ Prepared {len(X_train_arr)} training samples for intersection model: {np.sum(y_train_arr)} positive, {len(y_train_arr) - np.sum(y_train_arr)} negative")
        print(f"üìä Image shape: {X_train_arr.shape[1:]} (resized from {patch_size}x{patch_size} patches)")
    else:
        print(f"‚ö†Ô∏è No training samples prepared for intersection model.")
    return X_train_arr, y_train_arr

# Cell 6: Intersection Detector - Model Definition
def create_intersection_detector_model(input_shape=(32, 32, 1), learning_rate=0.0003):
    model = Sequential([
        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape), MaxPooling2D(pool_size=(2, 2)),
        Conv2D(64, kernel_size=(3, 3), activation='relu'), MaxPooling2D(pool_size=(2, 2)),
        Conv2D(128, kernel_size=(3, 3), activation='relu'), MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(128, activation='relu'), Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    return model

# Cell 7: Intersection Detector - Training Function
def train_intersection_detector(X_train, y_train, batch_size=16, epochs=100, model_input_shape=(32,32,1)): # batch_size from cell 10
    datagen = ImageDataGenerator(preprocessing_function=custom_augment_intersection, validation_split=0.20)
    
    model = create_intersection_detector_model(input_shape=model_input_shape) # LR is default in func
    model.summary()

    train_generator = datagen.flow(X_train, y_train, batch_size=batch_size, subset='training')
    validation_generator = datagen.flow(X_train, y_train, batch_size=batch_size, subset='validation')

    callbacks = [
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1), # patience from cell 10
        ModelCheckpoint(os.path.join(MODEL_DIR, "intersection_detector_checkpoint.weights.h5"), monitor='val_accuracy', save_best_only=True, save_weights_only=True, verbose=1)
    ]
    history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator, callbacks=callbacks, verbose=1)
    
    final_model_path = os.path.join(MODEL_DIR, "intersection_detector.keras") # Use .keras format
    model.save(final_model_path)
    print(f"‚úÖ Intersection Detector model saved to {final_model_path}")

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1); plt.plot(history.history['accuracy']); plt.plot(history.history['val_accuracy']); plt.title('Intersection Model Accuracy'); plt.legend(['Train', 'Val'])
    plt.subplot(1, 2, 2); plt.plot(history.history['loss']); plt.plot(history.history['val_loss']); plt.title('Intersection Model Loss'); plt.legend(['Train', 'Val'])
    plt.tight_layout(); plt.show()
    return model, history

# Cell 8: Intersection Detector - Execute Training
# Option 1: Load pre-saved data
X_train_intersect_path = '/content/drive/MyDrive/Soduku_project/X_train.npy'
y_train_intersect_path = '/content/drive/MyDrive/Soduku_project/y_train.npy'

load_from_files = os.path.exists(X_train_intersect_path) and os.path.exists(y_train_intersect_path)

if load_from_files:
    print("üîÑ Loading pre-saved intersection training data...")
    X_train_intersect = np.load(X_train_intersect_path)
    y_train_intersect = np.load(y_train_intersect_path)
    print(f"Debug: Loaded {len(X_train_intersect)} samples, {len(y_train_intersect)} labels for intersection model.")
else:
    print("üîÑ Pre-saved intersection data not found. Generating from scratch...")
    raw_images_intersect, raw_points_intersect = load_intersection_data(DATA_DIR_INTERSECTIONS)
    if raw_images_intersect:
        # Use global patch_size_intersection loaded from settings in Cell 3
        X_train_intersect, y_train_intersect = prepare_training_data_intersection(raw_images_intersect, raw_points_intersect, patch_size=patch_size_intersection)
    else:
        X_train_intersect, y_train_intersect = np.array([]), np.array([])

if len(X_train_intersect) > 0 and len(y_train_intersect) > 0:
    print(f"üìä Intersection Dataset Stats: Samples={len(X_train_intersect)}, Positives={np.sum(y_train_intersect)}, Negatives={len(y_train_intersect)-np.sum(y_train_intersect)}")
    print("üöÄ Starting Intersection Detector model training...")
    intersection_model, intersection_history = train_intersection_detector(X_train_intersect, y_train_intersect, patch_size_intersection)
else:
    print("‚ùå Cannot train Intersection Detector: No training data available.")
    intersection_model = None

# Cell 9: Intersection Detector - Save Final Model to Drive and Download
if intersection_model:
    drive_model_path_intersect = os.path.join(SAVED_MODELS_DRIVE_DIR, "intersection_detector.keras")
    intersection_model.save(drive_model_path_intersect) # Save again to ensure final state to Drive
    print(f"üíæ Intersection Detector model saved to Google Drive at: {drive_model_path_intersect}")
    
    # from google.colab import files # Download option
    # print("üì• Creating download link for your intersection_detector.keras model...")
    # files.download(drive_model_path_intersect)
else:
    print("üì¶ Intersection detector model not trained in this session. Skipping save to Drive/download.")


# Cell 10: Board Detector - Data Loading, Augmentation, and Preprocessing Functions
BOARD_INPUT_SIZE = 416  # CNN input size for board detector
BOARD_BOUNDING_BOX_PADDING = 0.05
BOARD_AUGMENTATION_FACTOR = 4
BOARD_BRIGHTNESS_RANGE = (0.6, 1.4)
BOARD_ZOOM_RANGE = (0.7, 1.3)

def board_load_training_data(data_dir):
    print("üîÑ Loading training data for Board Detector...")
    dat_files = glob.glob(os.path.join(data_dir, "*.dat"))
    images, bboxes, paths = [], [], []
    failed_count = 0
    for dat_file in tqdm(dat_files, desc="Processing board data files"):
        base_name = os.path.splitext(os.path.basename(dat_file))[0]
        img_file = None
        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:
            potential_img = os.path.join(data_dir, base_name + ext)
            if os.path.exists(potential_img): img_file = potential_img; break
        if not img_file: failed_count += 1; continue
        img = cv2.imread(img_file)
        if img is None: failed_count += 1; continue
        
        try:
            points = parse_dat_file_board(dat_file) # Uses utility function
            if len(points) < 4: failed_count += 1; continue
            corners = detect_grid_corners_board(points) # Uses utility function
            img_h, img_w = img.shape[:2]
            bbox = create_bounding_box_board(corners, img_w, img_h, BOARD_BOUNDING_BOX_PADDING) # Uses utility
            images.append(img); bboxes.append(bbox); paths.append(img_file)
        except Exception as e: print(f"Error processing {dat_file} for board: {e}"); failed_count +=1
    
    print(f"‚úÖ Board Detector: Successfully loaded {len(images)} samples. Failed: {failed_count}")
    return images, bboxes, paths

def board_augment_training_data(images, bounding_boxes, augmentation_factor=BOARD_AUGMENTATION_FACTOR):
    print(f"üîÑ Generating {augmentation_factor}x augmented data for Board Detector...")
    aug_images, aug_bboxes = list(images), list(bounding_boxes) # Keep originals

    for img, bbox in tqdm(zip(images, bounding_boxes), total=len(images), desc="Augmenting board data"):
        for _ in range(augmentation_factor):
            aug_img_iter = img.copy()
            aug_img_iter = apply_brightness_augmentation_board(aug_img_iter, BOARD_BRIGHTNESS_RANGE) # Utility
            aug_img_iter, aug_bbox_iter = apply_zoom_augmentation_board(aug_img_iter, bbox, BOARD_ZOOM_RANGE) # Utility

            if np.random.random() < 0.6: aug_img_iter = apply_gaussian_noise_board(aug_img_iter) # Utility
            if np.random.random() < 0.4: aug_img_iter = apply_blur_augmentation_board(aug_img_iter) # Utility
            if np.random.random() < 0.5: aug_img_iter = apply_contrast_augmentation_board(aug_img_iter) # Utility
            if np.random.random() < 0.5: aug_img_iter = apply_gamma_correction_board(aug_img_iter) # Utility
            
            x1,y1,x2,y2 = aug_bbox_iter
            if x2 > x1 and y2 > y1 and x1 >=0 and y1 >=0 and x2 <=1 and y2 <=1 : # Validate bbox
                aug_images.append(aug_img_iter); aug_bboxes.append(aug_bbox_iter)
    print(f"‚úÖ Board Detector: Generated {len(aug_images)} total samples after augmentation.")
    return aug_images, aug_bboxes

def board_preprocess_image(image, target_size=BOARD_INPUT_SIZE):
    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    h, w = img_rgb.shape[:2]; scale = target_size / max(h, w)
    new_w, new_h = int(w * scale), int(h * scale)
    resized = cv2.resize(img_rgb, (new_w, new_h))
    pad_x, pad_y = (target_size - new_w) // 2, (target_size - new_h) // 2
    padded = np.full((target_size, target_size, 3), 128, dtype=np.uint8) # Pad with mid-gray
    padded[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = resized
    return padded.astype(np.float32) / 255.0

def board_create_training_arrays(images, bounding_boxes, target_size=BOARD_INPUT_SIZE):
    print("üîÑ Preprocessing images and adjusting coordinates for Board Detector...")
    X, y = [], []
    for img, bbox in tqdm(zip(images, bounding_boxes), total=len(images), desc="Creating board arrays"):
        orig_h, orig_w = img.shape[:2]
        processed_img = board_preprocess_image(img, target_size)
        X.append(processed_img)

        # Transform bbox to match preprocessed image space
        x1_orig, y1_orig, x2_orig, y2_orig = bbox[0]*orig_w, bbox[1]*orig_h, bbox[2]*orig_w, bbox[3]*orig_h
        scale = target_size / max(orig_h, orig_w)
        new_w, new_h = int(orig_w * scale), int(orig_h * scale)
        pad_x, pad_y = (target_size - new_w) // 2, (target_size - new_h) // 2
        
        x1_prep = (x1_orig * scale + pad_x) / target_size
        y1_prep = (y1_orig * scale + pad_y) / target_size
        x2_prep = (x2_orig * scale + pad_x) / target_size
        y2_prep = (y2_orig * scale + pad_y) / target_size
        
        label = [max(0,min(1,x1_prep)), max(0,min(1,y1_prep)), max(0,min(1,x2_prep)), max(0,min(1,y2_prep)), 1.0] # Add confidence
        y.append(label)
    
    X_arr, y_arr = np.array(X), np.array(y)
    print(f"‚úÖ Board Detector: Created training arrays X:{X_arr.shape}, y:{y_arr.shape}")
    return X_arr, y_arr

def board_visualize_bounding_boxes(images, bboxes, paths, num_samples=6):
    fig, axes = plt.subplots(2, (num_samples + 1) // 2, figsize=(15, 10))
    axes = axes.flatten()
    indices = random.sample(range(len(images)), min(num_samples, len(images)))
    for i, idx in enumerate(indices):
        img = images[idx]; bbox = bboxes[idx]; path = paths[idx]
        img_rgb_bbox = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).copy()
        h,w = img.shape[:2]
        x1,y1,x2,y2 = int(bbox[0]*w), int(bbox[1]*h), int(bbox[2]*w), int(bbox[3]*h)
        cv2.rectangle(img_rgb_bbox, (x1,y1), (x2,y2), (255,0,0), 3)
        axes[i].imshow(img_rgb_bbox); axes[i].set_title(f'{os.path.basename(path)}\nBBoxNorm:({bbox[0]:.2f},{bbox[1]:.2f},{bbox[2]:.2f},{bbox[3]:.2f})', fontsize=8); axes[i].axis('off')
    plt.tight_layout(); plt.suptitle('Board Detector: Generated BBoxes (Red)', fontsize=14); plt.show()

# Cell 11: Board Detector - Model Definition
def create_sudoku_board_detector_model(input_size=BOARD_INPUT_SIZE, learning_rate=0.0005):
    model = Sequential([
        Conv2D(32, (3,3), activation='relu', input_shape=(input_size,input_size,3)), MaxPooling2D((2,2)),
        Conv2D(64, (3,3), activation='relu'), MaxPooling2D((2,2)),
        Conv2D(128, (3,3), activation='relu'), MaxPooling2D((2,2)),
        Conv2D(256, (3,3), activation='relu'), MaxPooling2D((2,2)),
        Conv2D(512, (3,3), activation='relu'), GlobalAveragePooling2D(),
        Dense(256, activation='relu'), Dropout(0.5),
        Dense(128, activation='relu'), Dropout(0.3),
        Dense(5, activation='sigmoid') # x1, y1, x2, y2, confidence
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae', 'mse'])
    return model

# Cell 12: Board Detector - Main Training Execution
print("üöÄ Starting Sudoku Board Detection Training Pipeline")
# Config from original Cell 10.7 (can be adjusted here)
BOARD_EPOCHS = 100
BOARD_BATCH_SIZE = 8
BOARD_LEARNING_RATE = 0.0005
BOARD_VALIDATION_SPLIT = 0.2
BOARD_EARLY_STOPPING_PATIENCE = 15
BOARD_REDUCE_LR_PATIENCE = 8
USE_BOARD_LR_SCHEDULE = True

raw_images_board, raw_bboxes_board, raw_paths_board = board_load_training_data(DATA_DIR_BOARD_DETECTOR)

if not raw_images_board:
    print("‚ùå No training data loaded for Board Detector. Skipping training.")
    board_detector_model_trained = None
else:
    board_visualize_bounding_boxes(raw_images_board, raw_bboxes_board, raw_paths_board)
    aug_images_board, aug_bboxes_board = board_augment_training_data(raw_images_board, raw_bboxes_board)
    X_board, y_board = board_create_training_arrays(aug_images_board, aug_bboxes_board)

    X_train_board, X_val_board, y_train_board, y_val_board = train_test_split(X_board, y_board, test_size=BOARD_VALIDATION_SPLIT, random_state=42)
    print(f"üìä Board Detector Training samples: {len(X_train_board)}, Validation samples: {len(X_val_board)}")

    board_detector_model_trained = create_sudoku_board_detector_model(learning_rate=BOARD_LEARNING_RATE)
    board_detector_model_trained.summary()

    callbacks_board = [
        EarlyStopping(monitor='val_loss', patience=BOARD_EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=1),
        ModelCheckpoint(os.path.join(MODEL_DIR, "best_sudoku_board_detector.keras"), monitor='val_loss', save_best_only=True, verbose=1)
    ]
    if USE_BOARD_LR_SCHEDULE:
        callbacks_board.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=BOARD_REDUCE_LR_PATIENCE, min_lr=1e-7, verbose=1))

    print(f"\nüöÄ Starting Board Detector OPTIMIZED training for {BOARD_EPOCHS} epochs...")
    history_board = board_detector_model_trained.fit(
        X_train_board, y_train_board,
        validation_data=(X_val_board, y_val_board),
        batch_size=BOARD_BATCH_SIZE, epochs=BOARD_EPOCHS, callbacks=callbacks_board, verbose=1
    )

    final_board_model_path_local = os.path.join(MODEL_DIR, "sudoku_board_detector.keras")
    final_board_model_path_drive = os.path.join(SAVED_MODELS_DRIVE_DIR, "sudoku_board_detector.keras")
    board_detector_model_trained.save(final_board_model_path_local)
    board_detector_model_trained.save(final_board_model_path_drive) # Also save best to drive
    print(f"üíæ Board Detector model saved to: {final_board_model_path_local} (local) and {final_board_model_path_drive} (Drive)")
    
    # Save the "best" model from checkpoint to final name on drive as well
    best_model_path_drive = os.path.join(SAVED_MODELS_DRIVE_DIR, "best_sudoku_board_detector.keras")
    if os.path.exists(os.path.join(MODEL_DIR, "best_sudoku_board_detector.keras")):
         # Check if Keras saved it as a file or directory
        if os.path.isfile(os.path.join(MODEL_DIR, "best_sudoku_board_detector.keras")):
            !cp "{os.path.join(MODEL_DIR, "best_sudoku_board_detector.keras")}" "{best_model_path_drive}"
            print(f"üíæ Best Board Detector model also copied to Drive: {best_model_path_drive}")
        elif os.path.isdir(os.path.join(MODEL_DIR, "best_sudoku_board_detector.keras")): # SavedModel format
            !cp -r "{os.path.join(MODEL_DIR, "best_sudoku_board_detector.keras")}" "{os.path.splitext(best_model_path_drive)[0]}" # Copy directory
            print(f"üíæ Best Board Detector model (SavedModel dir) also copied to Drive: {os.path.splitext(best_model_path_drive)[0]}")


    plt.figure(figsize=(15, 5))
    plt.subplot(1,3,1); plt.plot(history_board.history['loss']); plt.plot(history_board.history['val_loss']); plt.title('Board Model Loss (MSE)'); plt.legend(['Train', 'Val'])
    plt.subplot(1,3,2); plt.plot(history_board.history['mae']); plt.plot(history_board.history['val_mae']); plt.title('Board Model MAE'); plt.legend(['Train', 'Val'])
    if USE_BOARD_LR_SCHEDULE and 'lr' in history_board.history:
         plt.subplot(1,3,3); plt.plot(history_board.history['lr']); plt.title('Learning Rate'); plt.yscale('log')
    plt.tight_layout(); plt.show()

# Cell 13: Integrated Full Pipeline Test
print("\nüöÄ Starting Integrated Sudoku Processing Pipeline Test")

# --- Load Board Detector Model ---
board_detector_instance = BoardDetector() # Uses models.board_detector.BoardDetector
BOARD_DETECTOR_MODEL_PATH_DRIVE = os.path.join(SAVED_MODELS_DRIVE_DIR, "best_sudoku_board_detector.keras") # Recommended
if not os.path.exists(BOARD_DETECTOR_MODEL_PATH_DRIVE): # Fallback to non-best if best isn't there
    BOARD_DETECTOR_MODEL_PATH_DRIVE = os.path.join(SAVED_MODELS_DRIVE_DIR, "sudoku_board_detector.keras")

board_model_loaded = False
if os.path.exists(BOARD_DETECTOR_MODEL_PATH_DRIVE):
    board_model_loaded = board_detector_instance.load_model(BOARD_DETECTOR_MODEL_PATH_DRIVE)
    if board_model_loaded: print(f"‚úÖ Board Detector model loaded from: {BOARD_DETECTOR_MODEL_PATH_DRIVE}")
    else: print(f"‚ö†Ô∏è Board Detector model FAILED to load from: {BOARD_DETECTOR_MODEL_PATH_DRIVE}")
else:
    print(f"‚ö†Ô∏è Board Detector model not found at {BOARD_DETECTOR_MODEL_PATH_DRIVE}. Board detection will be skipped.")
if not board_model_loaded: board_detector_instance = None # Disable if not loaded

# --- Load Intersection Keras Model (for test_detector_intersections) ---
INTERSECTION_KERAS_MODEL_PATH_DRIVE = os.path.join(SAVED_MODELS_DRIVE_DIR, "intersection_detector.keras")
intersection_keras_model = None
if os.path.exists(INTERSECTION_KERAS_MODEL_PATH_DRIVE):
    try:
        intersection_keras_model = tf.keras.models.load_model(INTERSECTION_KERAS_MODEL_PATH_DRIVE)
        print(f"‚úÖ Keras Intersection model loaded from: {INTERSECTION_KERAS_MODEL_PATH_DRIVE}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading Keras Intersection model: {e}")
else:
    print(f"‚ö†Ô∏è Keras Intersection model not found at {INTERSECTION_KERAS_MODEL_PATH_DRIVE}. Intersection detection might fail.")

# --- Grid Reconstructor ---
grid_reconstructor_instance = RobustGridReconstructor() # from models.grid_reconstructor
cell_extractor_instance = RobustCellExtractor() # Already patched in Utilities cell

# --- Random Test Image Selection ---
test_image_path_final = None
if os.path.exists(TEST_IMAGE_DIR) and os.path.isdir(TEST_IMAGE_DIR):
    image_files_list_final = [f for f in os.listdir(TEST_IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    if image_files_list_final:
        # test_image_name_final = "image32.jpg" # For specific testing
        test_image_name_final = random.choice(image_files_list_final) # For random
        test_image_path_final = os.path.join(TEST_IMAGE_DIR, test_image_name_final)
        print(f"üîÑ Selected test image: {test_image_path_final}")
    else: print(f"‚ùå No image files found in {TEST_IMAGE_DIR}")
else: print(f"‚ùå Test image directory not found: {TEST_IMAGE_DIR}")

# --- Main Processing Logic ---
if test_image_path_final and os.path.exists(test_image_path_final) and intersection_keras_model:
    test_image_cv2 = cv2.imread(test_image_path_final)
    if test_image_cv2 is None:
        print(f"‚ùå ERROR: Failed to load image {test_image_path_final}")
    else:
        print("‚úÖ Test image loaded successfully for final pipeline.")
        plt.figure(figsize=(6,6)); plt.imshow(cv2.cvtColor(test_image_cv2, cv2.COLOR_BGR2RGB)); plt.title(f"Original: {test_image_name_final}"); plt.axis('off'); plt.show()

        board_bbox_coords = None # (x1, y1, x2, y2) absolute pixel coords
        board_detected_conf = 0.0

        # 1. Board Detection (Optional)
        if board_detector_instance:
            print("\n--- Running Board Detection ---")
            # BoardDetector.detect returns (x1, y1, x2, y2, conf) in absolute pixel coordinates
            detection_output_board = board_detector_instance.detect(test_image_cv2) 
            if detection_output_board:
                x1b, y1b, x2b, y2b, conf_b = detection_output_board
                board_detected_conf = conf_b
                print(f"Board detected with conf {conf_b:.3f} at: ({x1b}, {y1b}, {x2b}, {y2b})")
                if conf_b >= 0.5: # Confidence threshold
                    board_bbox_coords = (x1b, y1b, x2b, y2b)
                    img_viz_board = test_image_cv2.copy()
                    cv2.rectangle(img_viz_board, (x1b,y1b), (x2b,y2b), (0,255,0), 3)
                    plt.figure(figsize=(7,7)); plt.imshow(cv2.cvtColor(img_viz_board, cv2.COLOR_BGR2RGB)); plt.title(f"Detected Board (Conf: {conf_b:.2f})"); plt.axis('off'); plt.show()
                else: print(f"Board conf ({conf_b:.3f}) too low.")
            else: print("Board not detected by the model.")
        else: print("Board detector not available. Skipping board detection.")

        # 2. Intersection Detection (using test_detector_intersections)
        print("\n--- Running Intersection Detection ---")
        # Using corrections from original d_cell 12 / last cell
        # patch_size_intersection is from Cell 3 global settings
        all_intersections = test_detector_intersections(test_image_cv2, intersection_keras_model, 
                                                        patch_size=patch_size_intersection, 
                                                        x_correction=-5, y_correction=7) 
        print(f"Detected {len(all_intersections)} total raw intersections.")
        img_viz_intersections = test_image_cv2.copy()
        if board_bbox_coords: cv2.rectangle(img_viz_intersections, board_bbox_coords[:2], board_bbox_coords[2:], (255,165,0),2) # Orange for board
        for pt_i in all_intersections: cv2.circle(img_viz_intersections, pt_i, 3, (0,0,255), -1) # Red
        plt.figure(figsize=(7,7)); plt.imshow(cv2.cvtColor(img_viz_intersections, cv2.COLOR_BGR2RGB)); plt.title(f"All Detected Intersections ({len(all_intersections)})"); plt.axis('off'); plt.show()

        # 3. Filter Intersections by Board (if board detected reliably)
        intersections_for_grid = all_intersections
        use_board_filtered_intersections = False
        if board_bbox_coords and board_detector_instance: # Board was detected with high confidence
            print("\n--- Filtering Intersections based on Board ---")
            # filter_intersections expects points and (x1,y1,x2,y2) bbox
            filtered_by_board = board_detector_instance.filter_intersections(all_intersections, board_bbox_coords)
            print(f"{len(filtered_by_board)} intersections after board filtering.")
            
            img_viz_filtered = test_image_cv2.copy()
            cv2.rectangle(img_viz_filtered, board_bbox_coords[:2], board_bbox_coords[2:], (0,255,0),2) # Green board
            for pt_f in filtered_by_board: cv2.circle(img_viz_filtered, pt_f, 3, (255,0,0), -1) # Blue
            plt.figure(figsize=(7,7)); plt.imshow(cv2.cvtColor(img_viz_filtered, cv2.COLOR_BGR2RGB)); plt.title(f"Filtered Intersections ({len(filtered_by_board)}) by Board"); plt.axis('off'); plt.show()

            # Threshold from pipeline (e.g. 78 points for 82% of 100 - 18 missing)
            if len(filtered_by_board) >= 70: # Adjusted threshold, can be tuned
                intersections_for_grid = filtered_by_board
                use_board_filtered_intersections = True
                print(f"Using {len(intersections_for_grid)} board-filtered intersections for grid reconstruction.")
            else:
                print(f"Too few board-filtered intersections ({len(filtered_by_board)}). Using all {len(all_intersections)} intersections with multi-grid separation.")
        else:
            print("Proceeding with all intersections for multi-grid separation (board not reliably detected or detector unavailable).")
        
        # 4. Separate Multiple Grids (if present)
        print("\n--- Separating Multiple Grids (if present) ---")
        grid_point_sets = separate_sudoku_grids(intersections_for_grid) # Uses utility
        
        img_viz_multigrid = test_image_cv2.copy()
        colors = [(0,0,255),(0,255,0),(255,0,0),(255,255,0),(0,255,255),(255,0,255)]
        for i_set, g_set in enumerate(grid_point_sets):
            clr = colors[i_set % len(colors)]
            for pt_g in g_set: cv2.circle(img_viz_multigrid, pt_g, 5, clr, -1)
        plt.figure(figsize=(8,8)); plt.imshow(cv2.cvtColor(img_viz_multigrid, cv2.COLOR_BGR2RGB)); plt.title(f"Detected Grid Sets ({len(grid_point_sets)} found)"); plt.axis('off'); plt.show()

        primary_grid_points_for_reconstruction = []
        if grid_point_sets:
            primary_grid_points_for_reconstruction = max(grid_point_sets, key=len)
            print(f"Selected primary grid with {len(primary_grid_points_for_reconstruction)} points.")
        else:
            print("No grid sets found after separation, cannot reconstruct.")

        # 5. Grid Reconstruction
        final_reconstructed_grid = None
        if primary_grid_points_for_reconstruction:
            print("\n--- Running Grid Reconstruction ---")
            if use_board_filtered_intersections and board_bbox_coords:
                print("Attempting board-aware grid reconstruction...")
                # reconstruct_with_board_detection(self, points, image_shape, board_bbox, filtered_points)
                final_reconstructed_grid = grid_reconstructor_instance.reconstruct_with_board_detection(
                    all_intersections, # Original points for context
                    test_image_cv2.shape,
                    board_bbox_coords,
                    primary_grid_points_for_reconstruction # These are already filtered by board and selected
                )
                if not final_reconstructed_grid :
                    print("Board-aware reconstruction failed, falling back to standard.")
                    final_reconstructed_grid = grid_reconstructor_instance.reconstruct(primary_grid_points_for_reconstruction, test_image_cv2.shape)
            else:
                print("Attempting standard grid reconstruction...")
                final_reconstructed_grid = grid_reconstructor_instance.reconstruct(primary_grid_points_for_reconstruction, test_image_cv2.shape)
        
        # 6. Visualize Final Grid and Extract Cells
        if final_reconstructed_grid and len(final_reconstructed_grid) == 10 and all(len(r)==10 for r in final_reconstructed_grid):
            print(f"‚úÖ Grid reconstructed successfully: 10x10 points.")
            img_final_viz = test_image_cv2.copy()
            if board_bbox_coords: cv2.rectangle(img_final_viz, board_bbox_coords[:2], board_bbox_coords[2:], (255,165,0),2) # Board
            for i_gl in range(10): # Draw grid lines
                cv2.polylines(img_final_viz, [np.array(final_reconstructed_grid[i_gl], dtype=np.int32)], False, (0,255,0),2)
                cv2.polylines(img_final_viz, [np.array([final_reconstructed_grid[j_gl][i_gl] for j_gl in range(10)], dtype=np.int32)], False, (0,255,0),2)
            for pt_final_grid in primary_grid_points_for_reconstruction: cv2.circle(img_final_viz, pt_final_grid, 4, (0,0,255),-1) # Points used
            plt.figure(figsize=(10,8)); plt.imshow(cv2.cvtColor(img_final_viz,cv2.COLOR_BGR2RGB)); plt.title(f'Final Reconstructed Grid on {test_image_name_final}'); plt.axis('off'); plt.show()

            print("\n--- Extracting Cells ---")
            cell_images_final = cell_extractor_instance.extract(test_image_cv2, final_reconstructed_grid) # Uses patched extractor

            if cell_images_final and len(cell_images_final)==9 and all(len(row_c)==9 for row_c in cell_images_final):
                plt.figure(figsize=(9,9))
                for r_c in range(9):
                    for c_c in range(9):
                        plt.subplot(9,9, r_c*9 + c_c + 1)
                        cell_disp = cell_images_final[r_c][c_c]
                        if len(cell_disp.shape)==3 and cell_disp.shape[2]==1: cell_disp = cell_disp.squeeze(axis=2)
                        plt.imshow(cell_disp, cmap='gray'); plt.title(f'({r_c},{c_c})',fontsize=6); plt.axis('off')
                plt.suptitle(f'Extracted Cells from {test_image_name_final}', fontsize=14); plt.tight_layout(rect=[0,0,1,0.96]); plt.show()
            else: print(f"‚ùå Cell extraction failed or returned incorrect dimensions. Got {len(cell_images_final) if cell_images_final else 'None'} rows.")
        else:
            print(f"‚ùå Grid reconstruction failed or returned incorrect dimensions. Cannot extract cells.")
else:
    print("‚ùå ERROR: Test image not loaded or Keras Intersection model not available. Cannot run final pipeline.")

print("\nüéâ Sudoku Processing Test Complete!")
